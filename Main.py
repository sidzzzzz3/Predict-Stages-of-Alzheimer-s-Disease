# -*- coding: utf-8 -*-
"""Model 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TKQZV-hPgGFrCmg34SOF4NfTqInejK2F

# Environment Set up

**Set up combination**
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from os.path import join
from glob import glob
import numpy as np
import pandas as pd
import tensorflow as tf

import matplotlib.pyplot as plt
import PIL
import random

from tensorflow.keras.callbacks import ReduceLROnPlateau

import cv2 as cv

from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, \
    confusion_matrix, precision_score, recall_score, f1_score

from tensorflow.keras.models import Sequential
from keras.models import Sequential,load_model
from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau
from keras import backend as K

import copy
import warnings
warnings.filterwarnings('ignore')

from keras.preprocessing.image import load_img, img_to_array
import matplotlib

import seaborn as sns
from sklearn.utils import shuffle

from sklearn.decomposition import PCA
from math import ceil
from tensorflow.keras.preprocessing import image
# %matplotlib inline

import tensorflow.keras
import random
from tensorflow.keras.layers import *
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import *
from tensorflow.keras.losses import *
from tensorflow.keras.optimizers import *

from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

from matplotlib import cm
from matplotlib import axes

"""**Set up 1 - Data load - Cassie**"""

import os
from os.path import join
from glob import glob
import numpy as np
import pandas as pd
import tensorflow as tf

import matplotlib.pyplot as plt
import PIL
import random

from tensorflow.keras.callbacks import ReduceLROnPlateau

import cv2 as cv

from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, \
    confusion_matrix, precision_score, recall_score, f1_score
import seaborn as sns

"""**Set up 2 - Model 1 - Cassie**"""

from tensorflow.keras.models import Sequential
from keras.models import Sequential,load_model
from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D
from keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau
from keras import backend as K

"""**Set up 3**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os


import copy
import warnings
warnings.filterwarnings('ignore')

import cv2

from keras.preprocessing.image import load_img, img_to_array
import matplotlib
import matplotlib.pylab as plt
import numpy as np
import seaborn as sns
from sklearn.utils import shuffle

from sklearn.decomposition import PCA
from math import ceil
import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
# %matplotlib inline

"""**Set up 4 - accuracy/loss curves**"""

from matplotlib import pyplot as plt

"""**Set up 5 VGG**"""

import tensorflow.keras
import tensorflow as tf
import os
import numpy as np
import random
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.layers import *
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import *
from tensorflow.keras.losses import *
from tensorflow.keras.optimizers import *

"""**Set up 6 CNN**"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns

"""**Set up 7 Model Heatmap -- Cassie**"""

from matplotlib import cm
from matplotlib import axes

"""# Load Datasets"""

# define two functions to load 4 types of data
def import_data(directory, foldername, y, typename):
    """
    this funtion ...
    """
    data_df = pd.DataFrame({
        "X": sorted(glob(join(directory, foldername, "*"))),
        "y": y,
        "type": typename
    })
    return data_df

# set the directory of data import

directory = "/Users/cassieshen/Desktop/UCLA/2022 Spring/Math 156/156 Project/Alzheimer_s Dataset"

# build four dataframe
non_demented_df = import_data(directory, "./NonDemented", 0, "NonDemented")
very_mild_df = import_data(directory, "./VeryMildDemented", 1, "VeryMildDemented")
mild_df = import_data(directory, "./MildDemented", 2, "MildDemented")
moderate_df = import_data(directory, "./ModerateDemented", 3, "ModerateDemented")

"""# EDA section
1. count 4 types + bar plot
2. flip and add noise to increase the number "moderate" images
3. count 4 types + bar plot + show: flip, noise example images
4. average 4 type images + PCA
"""

# show class imbalance 1
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
size = [len(non_demented_df),len(very_mild_df),len(mild_df),len(moderate_df)]
ax.bar(('NonDemente','VeryMildDemented','MildDemented','ModerateDemented'),size)
plt.show

# create new folders named "NEW"
from os import listdir
from matplotlib import image
# load all images in a directory
# create flipped versions of an image
from PIL import Image
from matplotlib import pyplot

loaded_images = list()
for filename in listdir('Dataset/ModerateDemented'):
    image = Image.open('Dataset/ModerateDemented/' + filename)
    hoz_flip = image.transpose(Image.FLIP_LEFT_RIGHT)
    hoz_flip.save('NEW/F_MRI' + filename)#*****

# after combine newdata and original data step 1
# random noise
def random_noise(image,noise_num):
    img_noise = image
    rows,cols,chn = img_noise.shape

    for i in range(noise_num):
        x = np.random.randint(0 , rows)
        y = np.random.randint(0,cols)
        img_noise[x,y,:] = 255
    return img_noise
# salt and pepper
def sp_noise(noise_img, proportion):
    height,width = noise_img.shape[0],noise_img.shape[1]
    num = int(height*width*proportion)
    for i in range(num):
        w = random.randint(0,width - 1)
        h = random.randint(0, height - 1)
        if random.randint(0,1) == 0:
            noise_img[h,w] = 0
        else:
            noise_img[h,w] = 255
    return noise_img

# gaussian_noise
def gaussian_noise(img,mean,sigma):
    img = img / 255
    noise = np.random.normal(mean, sigma,img.shape)
    gaussian_out = img + noise
    gaussian_out = np.clip(gaussian_out,0,1)
    gaussian_out = np.uint8(gaussian_out*255)
    return gaussian_out
# clear newdata file then running below code 3 times
def convert(input_dir,output_dir):
    for filename in os.listdir(input_dir):
        path = input_dir + "/" + filename
        print("doing... ", path)
        noise_img = cv2.imread(path)
        img_noise = gaussian_noise(noise_img,0,0.3) # R N
        #img_noise = sp_noise(noise_img, 0.01)     # S P
        #img_noise = random_noise(noise_img,3000)  # Gaussin
        cv2.imwrite(output_dir + '/' + filename,img_noise)

data_path = 'Dataset/Moderate_Demented'
convert('Dataset/Moderate_Demented','NEW')
# repate step 1

"""After we did the #2, we could reimport the "moderate", and show #3, then do #4."""

#3.1
directory = "Dataset"

# build four dataframe
non_demented_df = import_data(directory, "./NonDemented", 0, "NonDemented")
very_mild_df = import_data(directory, "./VeryMildDemented", 1, "VeryMildDemented")
mild_df = import_data(directory, "./MildDemented", 2, "MildDemented")
moderate_df = import_data(directory, "./ModerateDemented", 3, "ModerateDemented")

#3.2
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
size = [len(non_demented_df),len(very_mild_df),len(mild_df),len(moderate_df)]
ax.bar(('NonDemente','VeryMildDemented','MildDemented','ModerateDemented'),size)
plt.show


#3.3
plt.figure(figsize=(20, 16))
# choose 4 types new datasets
images_path = ['/MildDemented/mild_4.jpg', '/ModerateDemented/F_MRImoderate 9.18.14 PM.jpg',
               '/NonDemented/non_2.jpg', '/VeryMildDemented/verymild_2.jpg']

# show MRI
categories = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']
for i in range(4):
    ax = plt.subplot(2, 2, i + 1)
    img = cv2.imread(directory + images_path[i])
    img = cv2.resize(img, (128, 128))
    plt.imshow(img)
    plt.title(categories[i])

# 4
# 4
# split dataset for Data mining
import splitfolders
splitfolders.ratio('Dataset', output="outcheck", seed=1345, ratio=(0.8,0,0.2))
IMG_HEIGHT = 128
IMG_WIDTH = 128
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
"./outcheck/train",
seed=123,
image_size=(IMG_HEIGHT, IMG_WIDTH),
batch_size=64
)

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
"./outcheck/test",
seed=123,
image_size=(IMG_HEIGHT, IMG_WIDTH),
batch_size=64
)

W = 128 # The default size for ResNet is 224 but resize to .5 to save memory size
H = 128 # The default size for ResNet is 224 but resize to .5 to save memory size

label_to_class = {
    'MildDemented': 0,
    'ModerateDemented': 1,
    'NonDemented': 2,
    'VeryMildDemented':3


}
class_to_label = {v: k for k, v in label_to_class.items()}
n_classes = len(label_to_class)

def get_images(dir_name='outcheck', label_to_class=label_to_class):
    """read images / labels from directory"""

    Images = []
    Classes = []

    for j in ['/train','/test']:
        for label_name in os.listdir(dir_name+str(j))[1:5]:
            cls = label_to_class[label_name]

            for img_name in os.listdir('/'.join([dir_name+str(j), label_name])):
                img = load_img('/'.join([dir_name+str(j), label_name, img_name]), target_size=(W, H))
                img = img_to_array(img)
                Images.append(img)
                Classes.append(cls)

    Images = np.array(Images, dtype=np.float32)
    Classes = np.array(Classes, dtype=np.float32)
    Images, Classes = shuffle(Images, Classes, random_state=0)

    return Images, Classes

# making n X m matrix
def img2np(dir_name, label,size = (64, 64)):
    # iterating through each file

    for j in ['/train','/test']:
        for label_name in os.listdir(dir_name+str(j)):
            if(label_name==label):
                for img_name in os.listdir('/'.join([dir_name+str(j), label_name])):
                    img = load_img('/'.join([dir_name+str(j), label_name, img_name]), target_size = size, color_mode = 'grayscale')
                    # covert image to a matrix
                    img_ts = img_to_array(img)
                    # turn that into a vector / 1D array
                    img_ts = [img_ts.ravel()]
                    try:
                        # concatenate different images
                        full_mat = np.concatenate((full_mat, img_ts))
                    except UnboundLocalError:
                        # if not assigned yet, assign one
                        full_mat = img_ts
    return full_mat

# run it on our folders
MildDemented_images = img2np('outcheck','MildDemented')
ModerateDemented_images = img2np('outcheck','ModerateDemented')
NonDemented_images = img2np('outcheck','NonDemented')
VeryMildDemented_images = img2np('outcheck','VeryMildDemented')

def find_mean_img(full_mat, title, size = (64, 64)):
    # calculate the average
    mean_img = np.mean(full_mat, axis = 0)
    # reshape it back to a matrix
    mean_img = mean_img.reshape(size)
    plt.imshow(mean_img, vmin=0, vmax=255, cmap='Greys_r')
    plt.title(f'Average {title}')
    plt.axis('off')
    plt.show()
    return mean_img

MildDemented_mean = find_mean_img(MildDemented_images, 'MildDemented')
ModerateDemented_mean = find_mean_img(ModerateDemented_images, 'ModerateDemented')
NonDemented_mean = find_mean_img(NonDemented_images, 'NonDemented')
VeryMildDemented_mean = find_mean_img(VeryMildDemented_images, 'VeryMildDemented')

def find_std_img(full_mat, title, size = (64, 64)):
    # calculate the average
    mean_img = np.std(full_mat, axis = 0)
    # reshape it back to a matrix
    mean_img = mean_img.reshape(size)
    plt.imshow(mean_img, vmin=0, vmax=255, cmap='Greys_r')
    plt.title(f'standard deviation {title}')
    plt.axis('off')
    plt.show()
    return mean_img


MildDemented_mean = find_std_img(MildDemented_images, 'MildDemented')
ModerateDemented_mean = find_std_img(ModerateDemented_images, 'ModerateDemented')
NonDemented_mean = find_std_img(NonDemented_images, 'NonDemented')
VeryMildDemented_mean = find_std_img(VeryMildDemented_images, 'VeryMildDemented')



n_total_images = Images.shape[0]

for target_cls in [0,1,2,3]:

    indices = np.where(Classes == target_cls)[0] # get target class indices on Images / Classes
    n_target_cls = indices.shape[0]
    label = class_to_label[target_cls]
    print(label, n_target_cls, n_target_cls*100/n_total_images)

    n_cols = 10 # # of sample plot
    fig, axs = plt.subplots(ncols=n_cols, figsize=(25, 3))

    for i in range(n_cols):

        axs[i].imshow(np.uint8(Images[indices[i]]))
        axs[i].axis('off')
        axs[i].set_title(label)

    plt.show()


def eigenimages(full_mat, title, n_comp = 0.8, size = (64, 64)):
    # fit PCA to describe n_comp * variability in the class
    pca = PCA(n_components = n_comp, whiten = True)
    pca.fit(full_mat)
    print('Number of PC: ', pca.n_components_)
    return pca

def plot_pca(pca, size = (64, 64)):
    # plot eigenimages in a grid
    n = pca.n_components_
    fig = plt.figure(figsize=(8, 8))
    r = int(n**.5)
    c = ceil(n/ r)
    for i in range(n):
        ax = fig.add_subplot(r, c, i + 1, xticks = [], yticks = [])
        ax.imshow(pca.components_[i].reshape(size),
                  cmap='Greys_r')
    plt.axis('off')
    plt.show()

MildDemented_mean = plot_pca(eigenimages(MildDemented_images, 'MildDemented'))
ModerateDemented_mean = plot_pca(eigenimages(ModerateDemented_images, 'ModerateDemented'))
NonDemented_mean = plot_pca(eigenimages(NonDemented_images, 'NonDemented'))
VeryMildDemented_mean = plot_pca(eigenimages(VeryMildDemented_images, 'VeryMildDemented'))

"""# Data Preparation

4 directory dataframe
1. split 4 types of data into train, test, validation.
2. Set parameters
3. concat the 4 dataframe

Split 4 types of data
"""

def random_split_data(data_df):
    """
    this function...
    """
    random_data = data_df.sample(frac=1)
    length = len(random_data)
    test = random_data[:int(0.2*length)]
    remaining = random_data[int(0.2*length):]
    length_2 = len(remaining)
    train = remaining[:int(0.8*length_2)]
    validation = remaining[int(0.8*length_2):]
    return test, train, validation

# we only use train here and split train dataset to 0.8/0.2 ratio of train/test,
# and in new train, we split it to 0.8/0.2 ration of train/validation


# split the four dataframe seperately
non_test, non_train, non_val = random_split_data(non_demented_df)
very_mild_test, very_mild_train, very_mild_val = random_split_data(very_mild_df)
mild_test, mild_train, mild_val = random_split_data(mild_df)
moderate_test, moderate_train, moderate_val = random_split_data(moderate_df)

"""Set parameters"""

Autotune = tf.data.experimental.AUTOTUNE

Image_size = [224, 224]
Batch_size = 64
Epochs = 50

"""Concat the 4 dataframe and randomize it"""

# concat the 4 data frame
train_total = pd.concat([non_train, very_mild_train, mild_train, moderate_train])
validation_total = pd.concat([non_val, very_mild_val, mild_val, moderate_val])
test_total = pd.concat([non_test, very_mild_test, mild_test, moderate_test])

# randomize them
train_total = train_total.sample(frac=1)
validation_total = validation_total.sample(frac=1)
test_total = test_total.sample(frac=1)

"""Build three functions to load image, create class array and extract X, y for each train, validation and test."""

# load image using the directory df we create before

def load_image(file):
    """
    this function is...
    """
    image = cv.imread(file)
    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
    # resize it to a same size
    image = cv.resize(image, (Image_size[0], Image_size[1]))
    return image

def encoding_array(y_number):
    """
    this function is...
    """
    class_array = [0,0,0,0]
    class_array[y_number] = 1
    return class_array

def data_preparation(data_set):
    """
    this function is...
    """
    data_set["X"] = data_set["X"].apply(load_image)
    data_set["y"] = data_set["y"].apply(encoding_array)
    X = np.stack(data_set["X"])
    y = np.stack(data_set["y"])
    return X, y

# create X, y for train, validation, test
X_train, y_train = data_preparation(train_total)
X_validation, y_validation = data_preparation(validation_total)
X_test, y_test = data_preparation(test_total)

"""# Model 1 -- NasNetMobile section

1. Model built
2. train
3. fit
4. accuracy line plot + loss line plot
5. predit
6. predit confusion matrix plot

"""

# import base model
base_model = tf.keras.applications.NASNetMobile(input_shape=(224, 224,3),
                         include_top=False, weights="imagenet")

# Freezing Layers and Train the last 6 layers
for layer in base_model.layers[:-6]:
    layer.trainable=False

# Building Model

model=Sequential()
model.add(base_model)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(4,activation='softmax'))

# Model Summary

model.summary()

def f1_score(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

# Revised from the original Binary to Categorical
METRICS = [
      tf.keras.metrics.CategoricalAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),
      tf.keras.metrics.AUC(name='auc'),
        f1_score,
]

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)

history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=Batch_size, epochs = Epochs, verbose = 1, callbacks=[lrd,mcp,es])

def plot_curve(history):


    fig, ax = plt.subplots(2, 1, figsize=(20, 20))
    ax = ax.ravel()

    ax[0].set_ylim([0.2,1])

    ax[0].plot(history.history["accuracy"],linewidth=3,color="#FFB81C",marker='o')
    ax[0].plot(history.history["val_accuracy"],linewidth=3,marker='o',color="#2D68C4")
    ax[0].set_title('Model {}'.format("Accuray"))
    ax[0].set_xlabel('Epochs')
    ax[0].set_ylabel("Accuracy")
    ax[0].legend(['Train', 'Validation'])

    ax[1].set_ylim([0.5,1.5])

    ax[1].plot(history.history["loss"],linewidth=3,color="#FFB81C",marker='o')
    ax[1].plot(history.history['val_' + "loss"],linewidth=3,
     marker='o', markeredgecolor='black',color="#2D68C4")
    ax[1].set_title('Model {}'.format("Loss"))
    ax[1].set_xlabel('Epochs')
    ax[1].set_ylabel("loss")
    ax[1].legend(['Train', 'Validation'])


plot_curve(history)

model.evaluate(X_test,y_test)

y_pred = model.predict(X_test)
y_pred_1 = np.squeeze(y_pred)
y_pred_2 = np.argmax(y_pred_1,axis=1)

y_new_test = y_test.argmax(axis=1)
matrix = confusion_matrix(y_new_test, y_pred_2, normalize="true")
matrix

def plot_confusion_matrix(matrix):
    cmap = cm.Blues
    fig, ax = plt.subplots(figsize=(6,6))
    ax.set_xticks([0, 1, 2, 3])
    ax.set_xticklabels(["Non", "Very Mild", "Mild", "Moderate"])
    ax.set_yticks([0, 1, 2, 3])
    ax.set_yticklabels(["Non", "Very Mild", "Mild", "Moderate"])
    h = ax.imshow(matrix, cmap = cmap, aspect="auto", vmin = 0, vmax = 1)
    for x in range(4):
        for y in range(4):
            value = float(format('%.2f' % matrix[y,x]))
            plt.text(x, y, value, verticalalignment = 'center', horizontalalignment = 'center')
    fig.colorbar(h)

plot_confusion_matrix(matrix)

"""# Model 2 -- VGG Section

1. Model built
2. train
3. fit
4. accuracy line plot + loss line plot
5. predit
6. predit confusion matrix plot
"""

# Image_size = (128, 128, 3) for this model, reload images and no need
# to resize it

# reload directory
train_total = pd.concat([non_train, very_mild_train, mild_train, moderate_train])
validation_total = pd.concat([non_val, very_mild_val, mild_val, moderate_val])
test_total = pd.concat([non_test, very_mild_test, mild_test, moderate_test])

# randomize them
train_total = train_total.sample(frac=1)
validation_total = validation_total.sample(frac=1)
test_total = test_total.sample(frac=1)

def load_image(file):
    """
    this function is...
    """
    image = cv.imread(file)
    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
    # since the original image size is (128, 128, 3), we don't resize the image
    # here
    return image

def data_preparation(data_set):
    """
    this function is...
    """
    data_set["X"] = data_set["X"].apply(load_image)
    X = np.stack(data_set["X"])
    y = np.stack(data_set["y"])
    return X, y

X_train, y_train = data_preparation(train_total)
X_validation, y_validation = data_preparation(validation_total)
X_test, y_test = data_preparation(test_total)

vgg16 = VGG16(weights='imagenet', input_shape=(128,128,3), include_top=False)
# Set all layers to non-trainable
for layer in vgg16.layers:
    layer.trainable = False
# Set the last 6 vgg layers to trainable
vgg16.layers[-2].trainable = True
vgg16.layers[-3].trainable = True
vgg16.layers[-4].trainable = True
vgg16.layers[-5].trainable = True
vgg16.layers[-6].trainable = True
vgg16.layers[-7].trainable = True

modelvgg = Sequential()
modelvgg.add(Input(shape=(128,128,3)))
modelvgg.add(vgg16)
modelvgg.add(Flatten())
modelvgg.add(Dropout(0.25))
modelvgg.add(Dense(128, activation='relu'))
modelvgg.add(Dropout(0.2))
modelvgg.add(Dense(4, activation='softmax'))

modelvgg.summary()

modelvgg.compile(optimizer=Adam(learning_rate=0.0001),
             loss='sparse_categorical_crossentropy',
             metrics=['sparse_categorical_accuracy'])

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

historyvgg = modelvgg.fit(X_train, y_train,validation_data=(X_validation,y_validation), batch_size=Batch_size, epochs = Epochs, verbose = 1, callbacks=[lrd,mcp,es])

modelvgg.evaluate(X_test,y_test)

from matplotlib import pyplot as plt

def plot_curve(history):


    fig, ax = plt.subplots(2, 1, figsize=(20, 20))
    ax = ax.ravel()

    ax[0].set_ylim([0.4,1.1])

    ax[0].plot(history.history["sparse_categorical_accuracy"],linewidth=3,color="#FFB81C",marker='o')
    ax[0].plot(history.history['val_' + "sparse_categorical_accuracy"],linewidth=3,marker='o',color="#2D68C4")
    ax[0].set_title('Model {}'.format("Accuray"))
    ax[0].set_xlabel('Epochs')
    ax[0].set_ylabel("Accuray")
    ax[0].legend(['Train', 'Validation'])

    ax[1].set_ylim([0,1.5])

    ax[1].plot(history.history["loss"],linewidth=3,color="#FFB81C",marker='o')
    ax[1].plot(history.history['val_' + "loss"],linewidth=3,
     marker='o', markeredgecolor='black',color="#2D68C4")
    ax[1].set_title('Model {}'.format("Loss"))
    ax[1].set_xlabel('Epochs')
    ax[1].set_ylabel("loss")
    ax[1].legend(['Train', 'Validation'])


plot_curve(historyvgg)

y_pred = modelvgg.predict(X_test)
y_pred_1 = np.squeeze(y_pred)
y_pred_2 = np.argmax(y_pred_1,axis=1)
matrix = confusion_matrix(y_test, y_pred_2, normalize="true")
matrix

plot_confusion_matrix(matrix)

"""# Model 3 section


1. Model built
2. train
3. fit
4. accuracy line plot + loss line plot
5. predit
6. predit confusion matrix plot
"""

# Image_size = (128, 128, 3) for this model, reload images and no need
# to resize it

# reload directory
train_total = pd.concat([non_train, very_mild_train, mild_train, moderate_train])
validation_total = pd.concat([non_val, very_mild_val, mild_val, moderate_val])
test_total = pd.concat([non_test, very_mild_test, mild_test, moderate_test])

# randomize them
train_total = train_total.sample(frac=1)
validation_total = validation_total.sample(frac=1)
test_total = test_total.sample(frac=1)

# load image using the directory df we create before

def load_image(file):
    """
    this function is...
    """
    image = cv.imread(file)
    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
    return image

def encoding_array(y_number):
    """
    this function is...
    """
    class_array = [0,0,0,0]
    class_array[y_number] = 1
    return class_array

def data_preparation(data_set):
    """
    this function is...
    """
    data_set["X"] = data_set["X"].apply(load_image)
    data_set["y"] = data_set["y"].apply(encoding_array)
    X = np.stack(data_set["X"])
    y = np.stack(data_set["y"])
    return X, y

# create X, y for train, validation, test
X_train, y_train = data_preparation(train_total)
X_validation, y_validation = data_preparation(validation_total)
X_test, y_test = data_preparation(test_total)

model3 = Sequential()

# Convolutional layer 1
model3.add(Conv2D(64,(7,7), input_shape=(128, 128, 3), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

#Convolutional layer 2
model3.add(Conv2D(128,(7,7), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 3
model3.add(Conv2D(128,(7,7), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 4
model3.add(Conv2D(256,(7,7), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

 # Convolutional layer 5
model3.add(Conv2D(256,(7,7), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 6
model3.add(Conv2D(512,(7,7), padding='same', activation='relu'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(2,2)))

model3.add(Flatten())

# Full connect layers

model3.add(Dense(units= 1024, activation='relu'))
model3.add(Dropout(0.25))
model3.add(Dense(units=512, activation='relu'))
model3.add(Dropout(0.25))
model3.add(Dense(units=4, activation='softmax'))



model3.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy',
                   metrics= ['categorical_accuracy'])

model3.summary()

model3_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)
model3_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)
model3_mcp = ModelCheckpoint(filepath = 'model3_weights.h5', monitor = 'val_categorical_accuracy',
                      save_best_only = True, verbose = 1)

history3 = model3.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=Batch_size, epochs=Epochs, verbose = 1,
                     callbacks=[model3_es, model3_rlr, model3_mcp])

def plot_curve(history):


    fig, ax = plt.subplots(2, 1, figsize=(20, 20))
    ax = ax.ravel()

    ax[0].set_ylim([0.4,1])

    ax[0].plot(history.history["categorical_accuracy"],linewidth=3,color="#FFB81C",marker='o')
    ax[0].plot(history.history['val_categorical_accuracy'],linewidth=3,marker='o',color="#2D68C4")
    ax[0].set_title('Model {}'.format("Accuray"))
    ax[0].set_xlabel('Epochs')
    ax[0].set_ylabel("Accuray")
    ax[0].legend(['Train', 'Validation'])

    ax[1].set_ylim([0,5])

    ax[1].plot(history.history["loss"],linewidth=3,color="#FFB81C",marker='o')
    ax[1].plot(history.history['val_' + "loss"],linewidth=3,
     marker='o', markeredgecolor='black',color="#2D68C4")
    ax[1].set_title('Model {}'.format("Loss"))
    ax[1].set_xlabel('Epochs')
    ax[1].set_ylabel("loss")
    ax[1].legend(['Train', 'Validation'])

plot_curve(history3)

model3.evaluate(X_test,y_test)

y_pred = model3.predict(X_test)
y_pred_1 = np.squeeze(y_pred)
y_pred_2 = np.argmax(y_pred_1,axis=1)

y_new_test = y_test.argmax(axis=1)
matrix = confusion_matrix(y_new_test, y_pred_2, normalize="true")
matrix

plot_confusion_matrix(matrix)

"""# Model 4"""

Image_size = [128, 128]
Batch_size = 64
Epochs = 5 # needed to change to 50 afterwards

train_total = pd.concat([non_train, very_mild_train, mild_train, moderate_train])
validation_total = pd.concat([non_val, very_mild_val, mild_val, moderate_val])
test_total = pd.concat([non_test, very_mild_test, mild_test, moderate_test])

# randomize them
train_total = train_total.sample(frac=1)
validation_total = validation_total.sample(frac=1)
test_total = test_total.sample(frac=1)

def load_image(file):
    """
    this function is...
    """
    image = cv.imread(file)
    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
    # resize it to a same size
    image = cv.resize(image, (Image_size[0], Image_size[1]))
    return image

def data_preparation(data_set):
    """
    this function is...
    """
    data_set["X"] = data_set["X"].apply(load_image)
    X = np.stack(data_set["X"])
    y = np.stack(data_set["y"])
    return X, y

X_train, y_train = data_preparation(train_total)
X_validation, y_validation = data_preparation(validation_total)
X_test, y_test = data_preparation(test_total)

from tensorflow.keras.applications.vgg16 import VGG16

vgg16 = VGG16(weights='imagenet', input_shape=(128,128,3), include_top=False)
# Set all layers to non-trainable
for layer in vgg16.layers:
    layer.trainable = False
# Set the last vgg block to trainable
vgg16.layers[-2].trainable = True
vgg16.layers[-3].trainable = True
vgg16.layers[-4].trainable = True
vgg16.layers[-5].trainable = True
vgg16.layers[-6].trainable = True
vgg16.layers[-7].trainable = True

model = Sequential()
model.add(Input(shape=(128,128,3)))
model.add(vgg16)
model.add(Flatten())
model.add(Dropout(0.25))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(4, activation='sigmoid'))

model.summary()

model.compile(optimizer=Adam(learning_rate=0.0001),
             loss='sparse_categorical_crossentropy',
             metrics=['sparse_categorical_accuracy'])

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

# history = model.fit(X_train, y_train,validation_data=(X_validation,y_validation), batch_size=Batch_size, epochs = Epochs, verbose = 1, callbacks=[lrd,mcp,es])

model.evaluate(X_test,y_test)

y_pred = model.predict(X_test)
y_pred_1 = np.squeeze(y_pred)
y_pred_2 = np.argmax(y_pred_1,axis=1)
matrix = confusion_matrix(y_test, y_pred_2, normalize="true")
matrix

cmap = cm.Blues
fig, ax = plt.subplots(figsize=(6,6))
ax.set_xticks([0, 1, 2, 3])
ax.set_xticklabels(["Non", "Very Mild", "Mild", "Moderate"])
ax.set_yticks([0, 1, 2, 3])
ax.set_yticklabels(["Non", "Very Mild", "Mild", "Moderate"])
h = ax.imshow(matrix, cmap = cmap, aspect="auto", vmin = 0, vmax = 1)
for x in range(4):
    for y in range(4):
        value = float(format('%.2f' % matrix[y,x]))
        plt.text(x, y, value, verticalalignment = 'center', horizontalalignment = 'center')
fig.colorbar(h)



cmap = cm.Blues
fig, ax = plt.subplots(figsize=(6,6))
ax.set_xticks([0, 1, 2, 3])
ax.set_xticklabels(["Non", "Very Mild", "Mild", "Moderate"])
ax.set_yticks([0, 1, 2, 3])
ax.set_yticklabels(["Non", "Very Mild", "Mild", "Moderate"])
h = ax.imshow(matrix, cmap = cmap, aspect="auto", vmin = 0, vmax = 1)
for x in range(4):
    for y in range(4):
        value = float(format('%.2f' % matrix[y,x]))
        plt.text(x, y, value, verticalalignment = 'center', horizontalalignment = 'center')
fig.colorbar(h)